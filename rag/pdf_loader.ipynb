{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ce4680",
   "metadata": {},
   "source": [
    "### RAG pipelines - Data Ingestion to VectorDB pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af6894b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cc01366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the pdf inside the directory\n",
    "from pathlib import Path\n",
    "\n",
    "def process_all_pdf(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing file: {pdf_file}\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata['source'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages from {pdf_file.name}.\")\n",
    "        except Exception as e:  \n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fe355ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files.\n",
      "\n",
      "Processing file: pdf_files\\document1.pdf\n",
      "Loaded 1 pages from document1.pdf.\n",
      "\n",
      "Processing file: pdf_files\\document2.pdf\n",
      "Loaded 1 pages from document2.pdf.\n",
      "\n",
      "Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "# Process all PDF in the data directory\n",
    "pdf_directory = \"./pdf_files\"\n",
    "all_pdf_documents = process_all_pdf(pdf_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25165525",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        splits = text_splitter.split_text(doc.page_content)\n",
    "        for i, split in enumerate(splits):\n",
    "            new_doc = doc.copy()\n",
    "            new_doc.page_content = split\n",
    "            new_doc.metadata['chunk_index'] = i\n",
    "            split_docs.append(new_doc)\n",
    "\n",
    "    print(f\"Total documents after splitting: {len(split_docs)}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6df8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents after splitting: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17528\\2632767502.py:15: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  new_doc = doc.copy()\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ae810",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96453d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9676425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}...\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. \\nLoaded embedding model: {self.model_name}. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a5bc428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1131.58it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. \n",
      "Loaded embedding model: all-MiniLM-L6-v2. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x16dd931c2f0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initialize Embedding Manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547438f8",
   "metadata": {},
   "source": [
    "### VectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Persistent directory means whatever is vector store files will be saved there.\"\"\"\n",
    "    def __init__(self, collection_name: str = \"document_embeddings\", persistent_directory: str = \"./vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            # Create persistent directory if not exists\n",
    "            print(f\"Initializing ChromaDB client with persistent directory: {self.persistent_directory}...\")\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persistent_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\" : \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized at {self.persistent_directory}.\")\n",
    "            print(f\"Existing collections: {self.collection.count()}\")\n",
    "            print(f\"ChromaDB collection '{self.collection_name}' initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store.\"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        #data preparation for chromadb\n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_texts=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate a unique ID for each document\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #prepare metadata and \n",
    "            metadata = dict(doc.metadata)  # Copy existing metadata\n",
    "            metadata['doc_index'] = i \n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_texts.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        #add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_texts,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(f\"Total documents in collection after addition: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff99ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client with persistent directory: ./vector_store...\n",
      "Vector store initialized at ./vector_store.\n",
      "Existing collections: 2\n",
      "ChromaDB collection 'document_embeddings' initialized successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x16ddb2dbcb0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16059614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-30T17:23:53+05:00', 'source': 'document1.pdf', 'file_path': 'pdf_files\\\\document1.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-30T17:23:53+05:00', 'trapped': '', 'modDate': \"D:20260130172353+05'00'\", 'creationDate': \"D:20260130172353+05'00'\", 'page': 0, 'file_type': 'pdf', 'chunk_index': 0}, page_content='Machine Learning Fundamentals\\nThis is the first dummy PDF document.\\nIt contains information about machine learning basics.\\nMachine learning algorithms learn patterns from data.\\nCommon types: Supervised, Unsupervised, Reinforcement Learning.\\nApplications include image recognition, NLP, and recommendations.\\nDeep learning uses neural networks for complex tasks.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-30T17:23:53+05:00', 'source': 'document2.pdf', 'file_path': 'pdf_files\\\\document2.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-30T17:23:53+05:00', 'trapped': '', 'modDate': \"D:20260130172353+05'00'\", 'creationDate': \"D:20260130172353+05'00'\", 'page': 0, 'file_type': 'pdf', 'chunk_index': 0}, page_content='Advanced Data Science Techniques\\nThis is the second dummy PDF document.\\nIt covers advanced topics in data science.\\nFeature engineering improves model performance.\\nCross-validation prevents overfitting.\\nHyperparameter tuning optimizes model accuracy.\\nEnsemble methods combine multiple models.')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0b8f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2 texts...\n",
      "Adding 2 documents to the vector store...\n",
      "Successfully added 2 documents to the vector store.\n",
      "Total documents in collection after addition: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract texts from chunks and generate embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b26359",
   "metadata": {},
   "source": [
    "### Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d269f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str,  top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Generating embedding for the query: {query}\")\n",
    "        query_embedding = self.embedding_manager.generate_embedding([query])[0]\n",
    "\n",
    "        print(f\"Query embedding generated. Retrieving top {top_k} similar documents...\")\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "\n",
    "        retrieved_docs = []\n",
    "        try:\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    similarity_score=1-distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'document': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                    print(f\"Retrieved Document {i+1}: ID={doc_id}, Similarity Score={similarity_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"No documents found above the similarity threshold of {score_threshold}.\")\n",
    "                return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aa659290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Retriever initialized.\n",
      "<__main__.RAGRetriever object at 0x0000016DDBF3C830>\n"
     ]
    }
   ],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)\n",
    "print(\"RAG Retriever initialized.\")\n",
    "print(rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "432e313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for the query: What is the Advanced Data Science\n",
      "Generating embeddings for 1 texts...\n",
      "Query embedding generated. Retrieving top 5 similar documents...\n",
      "Retrieved Document 1: ID=doc_c2c87997_1, Similarity Score=0.0044\n",
      "Retrieved Document 2: ID=doc_c34413e9_1, Similarity Score=0.0044\n",
      "Retrieved Document 3: ID=doc_69e0bd2a_1, Similarity Score=0.0044\n",
      "Retrieved Document 4: ID=doc_a977d94d_0, Similarity Score=-0.3227\n",
      "Retrieved Document 5: ID=doc_31c03682_0, Similarity Score=-0.3227\n",
      "No documents found above the similarity threshold of 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_c2c87997_1',\n",
       "  'document': 'Advanced Data Science Techniques\\nThis is the second dummy PDF document.\\nIt covers advanced topics in data science.\\nFeature engineering improves model performance.\\nCross-validation prevents overfitting.\\nHyperparameter tuning optimizes model accuracy.\\nEnsemble methods combine multiple models.',\n",
       "  'metadata': {'modDate': \"D:20260130172353+05'00'\",\n",
       "   'author': 'anonymous',\n",
       "   'moddate': '2026-01-30T17:23:53+05:00',\n",
       "   'format': 'PDF 1.3',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'source': 'document2.pdf',\n",
       "   'file_path': 'pdf_files\\\\document2.pdf',\n",
       "   'chunk_index': 0,\n",
       "   'page': 0,\n",
       "   'title': 'untitled',\n",
       "   'total_pages': 1,\n",
       "   'keywords': '',\n",
       "   'creationDate': \"D:20260130172353+05'00'\",\n",
       "   'creator': 'anonymous',\n",
       "   'doc_index': 1,\n",
       "   'creationdate': '2026-01-30T17:23:53+05:00',\n",
       "   'content_length': 291,\n",
       "   'subject': 'unspecified',\n",
       "   'trapped': ''},\n",
       "  'similarity_score': 0.004385411739349365,\n",
       "  'distance': 0.9956145882606506,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c34413e9_1',\n",
       "  'document': 'Advanced Data Science Techniques\\nThis is the second dummy PDF document.\\nIt covers advanced topics in data science.\\nFeature engineering improves model performance.\\nCross-validation prevents overfitting.\\nHyperparameter tuning optimizes model accuracy.\\nEnsemble methods combine multiple models.',\n",
       "  'metadata': {'content_length': 291,\n",
       "   'author': 'anonymous',\n",
       "   'trapped': '',\n",
       "   'doc_index': 1,\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'modDate': \"D:20260130172353+05'00'\",\n",
       "   'moddate': '2026-01-30T17:23:53+05:00',\n",
       "   'file_path': 'pdf_files\\\\document2.pdf',\n",
       "   'page': 0,\n",
       "   'source': 'document2.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationDate': \"D:20260130172353+05'00'\",\n",
       "   'creator': 'anonymous',\n",
       "   'total_pages': 1,\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.3',\n",
       "   'chunk_index': 0,\n",
       "   'creationdate': '2026-01-30T17:23:53+05:00',\n",
       "   'title': 'untitled',\n",
       "   'subject': 'unspecified'},\n",
       "  'similarity_score': 0.004385411739349365,\n",
       "  'distance': 0.9956145882606506,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_69e0bd2a_1',\n",
       "  'document': 'Advanced Data Science Techniques\\nThis is the second dummy PDF document.\\nIt covers advanced topics in data science.\\nFeature engineering improves model performance.\\nCross-validation prevents overfitting.\\nHyperparameter tuning optimizes model accuracy.\\nEnsemble methods combine multiple models.',\n",
       "  'metadata': {'creationdate': '2026-01-30T17:23:53+05:00',\n",
       "   'content_length': 291,\n",
       "   'page': 0,\n",
       "   'title': 'untitled',\n",
       "   'moddate': '2026-01-30T17:23:53+05:00',\n",
       "   'chunk_index': 0,\n",
       "   'modDate': \"D:20260130172353+05'00'\",\n",
       "   'total_pages': 1,\n",
       "   'subject': 'unspecified',\n",
       "   'author': 'anonymous',\n",
       "   'creator': 'anonymous',\n",
       "   'file_path': 'pdf_files\\\\document2.pdf',\n",
       "   'trapped': '',\n",
       "   'creationDate': \"D:20260130172353+05'00'\",\n",
       "   'format': 'PDF 1.3',\n",
       "   'keywords': '',\n",
       "   'source': 'document2.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 1,\n",
       "   'producer': 'ReportLab PDF Library - (opensource)'},\n",
       "  'similarity_score': 0.004385411739349365,\n",
       "  'distance': 0.9956145882606506,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag_retriever.retrieve(\"What is the purpose of this Machine Learning?\")\n",
    "rag_retriever.retrieve(\"What is the Advanced Data Science\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca45da4",
   "metadata": {},
   "source": [
    "### Integeraion VectorDB context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "685ed42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Rag pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "llm=ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=512)\n",
    "\n",
    "\n",
    "## Rag Function\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['document'] for doc in results]) if results else \"\"\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    ##generate the answer using Groq LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    # print(f\"Context for LLM:\\n{context}\\n\")\n",
    "    response=llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93a2e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for the query: What is Advanced Data Science?\n",
      "Generating embeddings for 1 texts...\n",
      "Query embedding generated. Retrieving top 3 similar documents...\n",
      "Retrieved Document 1: ID=doc_c2c87997_1, Similarity Score=0.0020\n",
      "Retrieved Document 2: ID=doc_c34413e9_1, Similarity Score=0.0020\n",
      "Retrieved Document 3: ID=doc_69e0bd2a_1, Similarity Score=0.0020\n",
      "No documents found above the similarity threshold of 0.0.\n",
      "Based on the provided context, Advanced Data Science refers to the application of advanced techniques in data science, which includes:\n",
      "\n",
      "1. Feature engineering to improve model performance\n",
      "2. Cross-validation to prevent overfitting\n",
      "3. Hyperparameter tuning to optimize model accuracy\n",
      "4. Ensemble methods to combine multiple models\n",
      "\n",
      "In essence, Advanced Data Science involves the use of sophisticated methods and techniques to extract insights and knowledge from data, improve model performance, and optimize results.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is Advanced Data Science?\", rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6ed9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for the query: Explain the concept of machine learning.\n",
      "Generating embeddings for 1 texts...\n",
      "Query embedding generated. Retrieving top 5 similar documents...\n",
      "Retrieved Document 1: ID=doc_54872660_0, Similarity Score=0.3278\n",
      "Retrieved Document 2: ID=doc_a977d94d_0, Similarity Score=0.3278\n",
      "Retrieved Document 3: ID=doc_31c03682_0, Similarity Score=0.3278\n",
      "Retrieved Document 4: ID=doc_c34413e9_1, Similarity Score=-0.1081\n",
      "Retrieved Document 5: ID=doc_69e0bd2a_1, Similarity Score=-0.1081\n",
      "No documents found above the similarity threshold of 0.3.\n",
      "Answer: Machine learning is a fundamental concept in the field of artificial intelligence that enables algorithms to learn patterns from data. This means that machine learning algorithms can automatically improve their performance on a task without being explicitly programmed for it.\n",
      "\n",
      "In essence, machine learning algorithms are designed to analyze and make predictions or decisions based on the data they are trained on. They can identify relationships, trends, and correlations within the data, allowing them to make accurate predictions or classifications.\n",
      "\n",
      "There are three primary types of machine learning:\n",
      "\n",
      "1. **Supervised Learning**: In this type of learning, the algorithm is trained on labeled data, where the correct output is already known. The algorithm learns to map inputs to outputs based on the labeled data.\n",
      "2. **Unsupervised Learning**: In this type of learning, the algorithm is trained on unlabeled data, and it must find patterns or relationships within the data on its own.\n",
      "3. **Reinforcement Learning**: In this type of learning, the algorithm learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
      "\n",
      "Machine learning has numerous applications, including:\n",
      "\n",
      "* **Image Recognition**: Machine learning algorithms can be trained to recognize objects, scenes, and activities within images.\n",
      "* **Natural Language Processing (NLP)**: Machine learning algorithms can be trained to understand and generate human language.\n",
      "* **Recommendations**: Machine learning algorithms can be trained to recommend products, services, or content based on user behavior and preferences.\n",
      "\n",
      "Deep learning is a subset of machine learning that uses neural networks to perform complex tasks. Neural networks are composed of layers of interconnected nodes (neurons) that process and transmit information. Deep learning algorithms can learn to recognize patterns and relationships within data by adjusting the connections between nodes.\n",
      "\n",
      "Overall, machine learning is a powerful tool that enables algorithms to learn from data and make accurate predictions or decisions. Its applications are vast and diverse, and it has the potential to transform numerous industries and aspects of our lives.\n",
      "Sources: [{'source': 'document1.pdf', 'page': 0, 'score': 0.3278001546859741, 'preview': 'Machine Learning Fundamentals\\nThis is the first dummy PDF document.\\nIt contains information about ma...'}, {'source': 'document1.pdf', 'page': 0, 'score': 0.3278001546859741, 'preview': 'Machine Learning Fundamentals\\nThis is the first dummy PDF document.\\nIt contains information about ma...'}, {'source': 'document1.pdf', 'page': 0, 'score': 0.3278001546859741, 'preview': 'Machine Learning Fundamentals\\nThis is the first dummy PDF document.\\nIt contains information about ma...'}]\n",
      "Confidence: 0.3278001546859741\n",
      "Context Preview: Machine Learning Fundamentals\n",
      "This is the first dummy PDF document.\n",
      "It contains information about machine learning basics.\n",
      "Machine learning algorithms learn patterns from data.\n",
      "Common types: Supervise\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Rag PipeLine Features\n",
    "\n",
    "def rag_advanced(query,retriever,llm,top_k=3,min_score=0.2,return_context=False):\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    context=\"\\n\\n\".join([doc['document'] for doc in results]) if results else \"\"\n",
    "    \n",
    "    if not context:\n",
    "        output = {\n",
    "            'answer': \"No relevant documents found.\",\n",
    "            'sources': [],\n",
    "            'confidence': 0.0,\n",
    "            'context': \"\"\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file',doc['metadata'].get('source','unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        \"preview\": doc['document'][:100] + '...'\n",
    "        } for doc in results]\n",
    "    \n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    ##generate the answer using Groq LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response=llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence,\n",
    "        'context': context\n",
    "    }\n",
    "    return output\n",
    "#Example \n",
    "result = rag_advanced(\"Explain the concept of machine learning.\", rag_retriever, llm, top_k=5, min_score=0.3, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce929d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f353035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6ccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
